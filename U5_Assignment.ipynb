{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Daniil Krechko | 12149099 | 25.05.2023, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 5 – Language Modeling with LSTM (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> N. Rekabsaz, B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, M. Abbass, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 16-05-2023\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u5_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u5_utils.py</code> need to be installed.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.9 (✓)\n",
      "Installed numpy version: 1.23.5 (✓)\n",
      "Installed pandas version: 2.0.1 (✓)\n",
      "Installed PyTorch version: 1.13.0 (✓)\n"
     ]
    }
   ],
   "source": [
    "import u5_utils as u5\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u5.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u5.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Training and Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Setup the data set using the same parameter settings as in the main exercise notebook but with the changes mentioned below.</li>\n",
    "            <li>Change the batch size in the initial parameters to $64$ and observe its effect on the created batches. Explain how the corpora are transformed into batches.</li>\n",
    "            <li>Use a seed of $23$.</li>\n",
    "            <li>For a specific sequence in <code>val_data_splits</code> (e.g., index $15$), print the corresponding words of its first 25 wordIDs.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of tokens in dictionary 10001\n",
      "wordID of a word in the dictionary: 1203\n",
      "A word in the dictionary based on its wordID: 'says'\n",
      "Train data: number of tokens 929589\n",
      "Validation data: number of tokens 73760\n",
      "Test data: number of tokens 82430\n",
      "\n",
      "Train data split shape: torch.Size([14524, 64])\n",
      "Validation data split shape: torch.Size([1152, 64])\n",
      "Test data batchified shape: torch.Size([1287, 64])\n",
      "['<unk>', 'video', 'of', 'in', 'and', 'as', 'accounts', 'crude', 'marine', 'it', 'vested', 'on', '<eos>', \"'re\", 'said', 'of', 'is', 'the', 'looked', 'sales', '<unk>', 'a', 'as', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "data_path = os.path.join(\"resources\", \"penn\")\n",
    "save_path = \"model.pt\" # path to save the final model\n",
    "\n",
    "# Training & evaluation parameters\n",
    "train_batch_size = 64 # batch size for training\n",
    "eval_batch_size = 64 # batch size for validation/test\n",
    "max_seq_len = 40 # sequence length\n",
    "\n",
    "# Random seed to facilitate reproducibility\n",
    "torch.manual_seed(23)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_corpus = u5.Corpus(os.path.join(data_path, \"train.txt\"))\n",
    "valid_corpus = u5.Corpus(os.path.join(data_path, \"valid.txt\"))\n",
    "test_corpus = u5.Corpus(os.path.join(data_path, \"test.txt\"))\n",
    "\n",
    "dictionary = u5.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print(f\"Number of tokens in dictionary {ntokens}\")\n",
    "\n",
    "print(f\"wordID of a word in the dictionary: {dictionary.word2idx['book']}\")\n",
    "print(f\"A word in the dictionary based on its wordID: '{dictionary.idx2word[854]}'\")\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print(f\"Train data: number of tokens {len(train_data)}\")\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print(f\"Validation data: number of tokens {len(valid_data)}\")\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print(f\"Test data: number of tokens {len(test_data)}\")\n",
    "\n",
    "print()\n",
    "train_data_splits = u5.batchify(train_data, train_batch_size, device)\n",
    "print(f\"Train data split shape: {train_data_splits.shape}\")\n",
    "\n",
    "val_data_splits = u5.batchify(valid_data, eval_batch_size, device)\n",
    "print(f\"Validation data split shape: {val_data_splits.shape}\")\n",
    "\n",
    "test_data_splits = u5.batchify(test_data, eval_batch_size, device)\n",
    "print(f\"Test data batchified shape: {test_data_splits.shape}\")\n",
    "\n",
    "indexes = val_data_splits[15, :25]\n",
    "words = []\n",
    "for i in indexes:\n",
    "    words.append(dictionary.idx2word[i])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the shape of train, validation and test sets have changed (value of dimensions 1 and 2 inccreased by 2 times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Copy the implementation of <code>LM_LSTMModel</code> from the main exercise notebook but make the following changes:</li>\n",
    "            <ul>\n",
    "                <li>Add an integer parameter to <code>LM_LSTMModel</code>'s initialization, called <code>num_layers</code> which indicates the number of (vertically) stacked LSTM blocks. Hint: PyTorch's LSTM implementation directly supports this, so you simply have to set it when creating the LSTM instance (see parameter <code>num_layers</code> in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">documentation</a>).</li>\n",
    "                <li>Add a new bool parameter to <code>LM_LSTMModel</code>'s initialization, called <code>tie_weights</code>. Extend the implementation of <code>LM_LSTMModel</code> such that if <code>tie_weights</code> is set to <code>True</code>, the model ties/shares the parameters of <code>encoder</code> with the ones of <code>decoder</code>. Consider that <code>encoder</code> and <code>decoder</code> still remain separate components but their parameters are now the same (shared). This process is called <i>weight tying</i>. Feel free to search the internet for relevant resources and implementation hints.</li>\n",
    "            </ul>\n",
    "            <li>Create four models:</li>\n",
    "            <ul>\n",
    "                <li>1 layer and without weight tying</li>\n",
    "                <li>1 layer and with weight tying</li>\n",
    "                <li>2 layers and without weight tying</li>\n",
    "                <li>2 layers and with weight tying</li>\n",
    "            </ul>\n",
    "            <li>Compare the number of parameters of the models and report your observations.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model 1 total trainable parameters: 4332001\n",
      "Model 2: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model 2 total trainable parameters: 2331801\n",
      "Model 3: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model 3 total trainable parameters: 4653601\n",
      "Model 4: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model 4 total trainable parameters: 2653401\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "class LM_LSTMModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhid, num_layers, tie_weights: bool):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp) # matrix E in the figure\n",
    "        self.rnn = torch.nn.LSTM(ninp, nhid, num_layers)\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken) # matrix U in the figure\n",
    "        if tie_weights is True:\n",
    "            self.decoder.weight = self.encoder.weight       \n",
    "    \n",
    "    def forward(self, input, hidden=None, return_logs=True):\n",
    "        #ipdb.set_trace()\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        if return_logs:\n",
    "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
    "        else:\n",
    "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
    "        \n",
    "        return y_hat, last_hidden\n",
    "    \n",
    "# Model parameters\n",
    "emsize = 200  # size of word embeddings\n",
    "nhid = 200  # number of hidden units per layer\n",
    "\n",
    "model_1 = LM_LSTMModel(ntokens, emsize, nhid, num_layers = 1, tie_weights = False)\n",
    "model_1.to(device)\n",
    "\n",
    "print(f\"Model 1: {model_1}\")\n",
    "print(f\"Model 1 total trainable parameters: {sum(p.numel() for p in model_1.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model_2 = LM_LSTMModel(ntokens, emsize, nhid, num_layers = 1, tie_weights = True)\n",
    "model_2.to(device)\n",
    "\n",
    "print(f\"Model 2: {model_2}\")\n",
    "print(f\"Model 2 total trainable parameters: {sum(p.numel() for p in model_2.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model_3 = LM_LSTMModel(ntokens, emsize, nhid, num_layers = 2, tie_weights = False)\n",
    "model_3.to(device)\n",
    "\n",
    "print(f\"Model 3: {model_3}\")\n",
    "print(f\"Model 3 total trainable parameters: {sum(p.numel() for p in model_3.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model_4 = LM_LSTMModel(ntokens, emsize, nhid, num_layers = 2, tie_weights = True)\n",
    "model_4.to(device)\n",
    "\n",
    "print(f\"Model 4: {model_4}\")\n",
    "print(f\"Model 4 total trainable parameters: {sum(p.numel() for p in model_4.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are using weight tying the total amount of trainable parameters is decreased (by 2 times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3. [30 Points]</b>\n",
    "    <ul>\n",
    "        <li>Using the same setup as in the main lecture/exercise notebook, train all four models for $5$ epochs.</li>\n",
    "        <li>Using <code>ipdb</code>, look inside the <code>forward</code> function of <code>LM_LSTMModel</code> during training. Check the forward process from input to output particularly by looking at the shapes of tensors. Report the shape of all tensors used in <code>forward</code>. Try to translate the numbers into batches $B$ and sequence length $L$. For instance, if we know that the batch size is $B=32$, a tensor of shape $(32, 128, 3)$ can be interpreted as a batch of $32$ sequences with $3$ channels of size $L=128$. Thus, this tensor can be translated into $(32, 128, 3) \\rightarrow (B, L, 3)$. Look at the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">official documentation</a> to understand the order of the dimensions.</li>\n",
    "        <li>Evaluate the models. Compare the performances of all four models on the train, validation and test set (for the test set, use the best model according to the respective validation set performance), and report your observations. To do so, create a plot showing the following curves:</li>\n",
    "        <ul>\n",
    "            <li>Loss on each current training batch before every model update step as function of epochs</li>\n",
    "            <li>Loss on the validation set at every epoch</li>\n",
    "        </ul>\n",
    "        <li>Comment on the results!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dictionary: u5.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, train_data_splits,\n",
    "          clipping: float, learning_rate: float, print_interval: int, epoch: int,\n",
    "          criterion: torch.nn.Module = torch.nn.NLLLoss()):\n",
    "    \"\"\"\n",
    "    Train the model. Training mode turned on to enable dropout.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (train_data_splits.size(0) - 1) // max_seq_len\n",
    "    \n",
    "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
    "        batch_data, batch_targets = u5.get_batch(train_data_splits, i, max_seq_len)\n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        # Don't forget it! Otherwise, the gradients are summed together!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Repackaging batches only keeps the value of start_hidden and disconnects its computational graph.\n",
    "        # If repackaging is not done the, gradients are calculated from the current point to the beginning\n",
    "        # of the sequence which becomes computationally too expensive.\n",
    "        if start_hidden is not None:\n",
    "            start_hidden = u5.repackage_hidden(start_hidden)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "        \n",
    "        # Loss computation & backward pass\n",
    "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "        loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # The last hidden states of the current step is set as the start hidden state of the next step.\n",
    "        # This passes the information of the current batch to the next batch.\n",
    "        start_hidden = last_hidden\n",
    "        \n",
    "        # Clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        \n",
    "        # Updating parameters using SGD\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_i % print_interval == 0 and batch_i > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            throughput = elapsed * 1000 / print_interval\n",
    "            print(f\"| epoch {epoch:3d} | {batch_i:5d}/{n_batches:5d} batches | lr {learning_rate:02.2f} | ms/batch {throughput:5.2f} \"\n",
    "                  f\"| loss {cur_loss:5.2f} | perplexity {math.exp(cur_loss):8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "losses = []\n",
    "mod_names = [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"]            \n",
    "for model in models:\n",
    "    epochs = 5  # total number of training epochs\n",
    "    print_interval = 25  # print report statistics every x batches\n",
    "    lr = 20  # initial learning rate\n",
    "    clipping = 0.25  # gradient clipping\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    train_losts = []\n",
    "    val_losts = []\n",
    "    best_val_loss = None\n",
    "\n",
    "    # Loop over epochs.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "        val_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "        \n",
    "        train_losts.append(train_loss)\n",
    "        val_losts.append(val_loss)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "              f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] = lr\n",
    "    losses.append((train_losts, val_losts))\n",
    "plt.figure(figsize=(10,5))\n",
    "for loss, mod_name in zip(losses, mod_names):\n",
    "    plt.plot(loss[0], label=f'{mod_name} Training Loss')\n",
    "    plt.plot(loss[1], label=f'{mod_name} Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see on the plot the best model is model 3. It constantly decrease its loss. Model 1 and model 2 are less effective than models 3 and 4. Model 1(without weight tying) has a lower loss overall, while the function is not monotonous, in contras model 2(applying weight tying) starts from higher loss but constantly decreases it, so it is more stable. But overall, models with 2 stack layers (model 3 and 4) show better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Generation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4. [30 Points]</b>\n",
    "    <p>\n",
    "    Copy the language generation code from the main exercise notebook and perform the following tasks:\n",
    "    </p>\n",
    "        <ul>\n",
    "            <li>Compare all four previous models by generating $12$ words that append the starting word <tt>\"despite\"</tt>.</li>\n",
    "            <li>For each model, retrieve the top $10$ wordIDs with the highest probabilities from the generated probability distribution (<code>prob_dist</code>) following the starting word <tt>\"despite\"</tt>. Fetch the corresponding words of these wordIDs. Do you observe any specific linguistic characteristic common between these words?</li>\n",
    "            <li>The implementation in the main exercise notebook is based on sampling. Implement a second deterministic variant based on the <i>top-1</i> approach. In this particular variant, the generated word is the word with the highest probability in the predicted probability distribution. Repeat the same procedure as before (i.e., generate $12$ words that append the starting word <tt>\"despite\"</tt>).</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "despite <unk> recently <eos> the <unk> took three with funny they do n't\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "GENERATION_LENGTH = 12\n",
    "START_WORD = \"despite\"\n",
    "\n",
    "start_hidden = None\n",
    "START_WORD = START_WORD.lower()\n",
    "    \n",
    "generated_text = START_WORD\n",
    "with torch.no_grad():\n",
    "    wordid_input = dictionary.word2idx[START_WORD]\n",
    "    for i in range(0, GENERATION_LENGTH):\n",
    "        data = u5.batchify(torch.tensor([wordid_input]), 1, device)\n",
    "        \n",
    "        y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
    "        \n",
    "        prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze())\n",
    "        wordid_input = prob_dist.sample()\n",
    "        word_generated = dictionary.idx2word[wordid_input]\n",
    "        \n",
    "        generated_text += \" \" + word_generated\n",
    "        \n",
    "        start_hidden = last_hidden\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
